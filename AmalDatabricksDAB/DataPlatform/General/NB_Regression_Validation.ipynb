{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fda0d1-1c8d-40e3-ba3f-91a90bde161c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./NB_Configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ff3ee2-8bdc-479e-8770-f0b670b751c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"use catalog {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99d45a1-5d2c-4bb6-8413-e74f6b4c21d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Init"
    }
   },
   "outputs": [],
   "source": [
    "# Source_list = ['Silver.Financial_Account','Silver.Financial_Account_Party','Silver.Financial_Account_Balance']\n",
    "# Destination_list = ['crm.Financial_Account','crm.Financial_Account_Party','crm.Financial_Account_Balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d62330-d842-48cf-9eaf-a251eeab0970",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Init - Update"
    }
   },
   "outputs": [],
   "source": [
    "Source_list = [\n",
    "\"Silver.financial_account_party\",\n",
    "\"Silver.service_xref\",\n",
    "\"Silver.customer_master\",\n",
    "\"Silver.financial_account_address\",\n",
    "\"Silver.financial_account\",\n",
    "\"Silver.branch\",\n",
    "\"Silver.financial_account_service\",\n",
    "\"Silver.financial_account_fee\",\n",
    "\"Silver.product\",\n",
    "\"Silver.financial_account_balance\"]\n",
    "\n",
    "\n",
    "### CRM Tables\n",
    "Destination_list= [\n",
    "\"CRM.financial_account_party\",\n",
    "\"CRM.service_xref\",\n",
    "\"CRM.account\",\n",
    "\"CRM.financial_account_address\",\n",
    "\"CRM.financial_account\",\n",
    "\"CRM.branch\",\n",
    "\"CRM.financial_account_service\",\n",
    "\"CRM.financial_account_fee\",\n",
    "\"CRM.product\",\n",
    "\"CRM.financial_account_balance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afe79ff-8019-401b-83e5-6ef4f24c4112",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Column Existence Check"
    }
   },
   "outputs": [],
   "source": [
    "def columns_match(source_table, destination_table, source_table_info, destination_table_info):\n",
    "    result_df = []\n",
    "\n",
    "    source_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in source_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "    destination_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in destination_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "    source_diff = list(set(source_columns) - set(destination_columns))\n",
    "    destination_diff = list(set(destination_columns) - set(source_columns))\n",
    "\n",
    "    if len(source_diff) > 0 or len(destination_diff) > 0:\n",
    "        description = f\"Columns in {source_table} are not present in {destination_table}\"\n",
    "        result_df.append((source_table, destination_table, 'Column Existence Check', 'NA', len(source_diff), len(destination_diff), False, description))\n",
    "    else:\n",
    "        description = f\"Columns in {source_table} are present in {destination_table}\"\n",
    "        result_df.append((source_table, destination_table, 'Column Existence Check', 'NA', len(source_diff), len(destination_diff), True, description))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66896fb2-4c1b-4e05-bba4-f1eda9646c33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "To get the curated data"
    }
   },
   "outputs": [],
   "source": [
    "datatype_dict= {'string':'''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null and {column_name} != '' and {column_name} != ' ' ''', \n",
    "'timestamp':  '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null''',\n",
    "'date':  '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null''', \n",
    "'integer': '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null and {column_name} >0''',\n",
    "'float': '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null and {column_name} >0''',\n",
    "'double': '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null and {column_name} >0''',\n",
    "'decimal': '''select '{system1}' as System, count(*) as {column_name}_count from {table1} where {column_name} is not null and {column_name} >0'''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c88279-c6c8-44f9-8b18-20a121da8ee6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "To get the nulls/blanks"
    }
   },
   "outputs": [],
   "source": [
    "blanks_nulls = {'string':'''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is  null and {column_name} == '' and {column_name} == ' ' ''',\n",
    "'timestamp':  '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is null''',\n",
    "'date':  '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is  null''',\n",
    "'integer': '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is null and {column_name} ==0''',\n",
    "'float': '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is null and {column_name} ==0''',\n",
    "'double': '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is null and {column_name} ==0''',\n",
    "'decimal': '''select '{system}' as System, count(*) as {column_name}_count from {table} where {column_name} is null and {column_name} ==0'''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3e749dd-227e-4b95-83b3-cc286ee75e44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataValidation"
    }
   },
   "outputs": [],
   "source": [
    "def datavalidation(source_table, destination_table, source_table_info, destination_table_info):\n",
    "    result_df = []\n",
    "\n",
    "    source_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in source_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "    destination_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in destination_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    destination_schema = spark.table(destination_table).schema\n",
    "\n",
    "    source_columns_types = {field.name: field.dataType for field in source_schema}\n",
    "    destination_columns_types = {field.name: field.dataType for field in destination_schema}\n",
    "\n",
    "    for col in destination_columns:\n",
    "        if col in source_columns:\n",
    "            source_type = source_columns_types[col]\n",
    "            destination_type = destination_columns_types[col]\n",
    "\n",
    "            source_type_str = str(source_type).split('Type')[0].lower()\n",
    "            destination_type_str = str(destination_type).split('Type')[0].lower()\n",
    "\n",
    "            if source_type_str in datatype_dict and destination_type_str in datatype_dict:\n",
    "                source_query = datatype_dict[source_type_str].format(system1='source', column_name=col, table1=source_table)\n",
    "                destination_query = datatype_dict[destination_type_str].format(system1='destination', column_name=col,table1=destination_table)\n",
    "\n",
    "                source_count = spark.sql(source_query).collect()[0][1]\n",
    "                destination_count = spark.sql(destination_query).collect()[0][1]\n",
    "\n",
    "                if source_count == destination_count:\n",
    "                    description = f\"Row count for column {col} matches between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_count, destination_count, True, description))\n",
    "                else:\n",
    "                    description = f\"Row count for column {col} does not match between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_count, destination_count, False, description))\n",
    "            else:\n",
    "                description = f\"Data type for column {col} is not supported\"\n",
    "                result_df.append((source_table, destination_table, 'Data Validation', col, None, None, False, description))\n",
    "        else:\n",
    "            description = f\"Column {col} not found in {source_table}\"\n",
    "            result_df.append((source_table, destination_table, 'Data Validation', col, None, None, False, description))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0f329c-252a-43bd-be6f-7c291eb916f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ": DataValidation NonNulls , Nulls, Blanks, Zeroes- Update"
    }
   },
   "outputs": [],
   "source": [
    "def datavalidation_nulls_blanks(source_table, destination_table, source_table_info, destination_table_info):\n",
    "    \n",
    "    result_df = []\n",
    "\n",
    "    source_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in source_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "    destination_columns = [\n",
    "        row[\"column_name\"]\n",
    "        for row in destination_table_info.select(\"column_name\").distinct().collect()\n",
    "    ]\n",
    "\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    destination_schema = spark.table(destination_table).schema\n",
    "\n",
    "    source_columns_types = {field.name: field.dataType for field in source_schema}\n",
    "    destination_columns_types = {field.name: field.dataType for field in destination_schema}\n",
    "\n",
    "    for col in destination_columns:\n",
    "        if col in source_columns:\n",
    "            source_type = source_columns_types[col]\n",
    "            destination_type = destination_columns_types[col]\n",
    "\n",
    "            source_type_str = str(source_type).split('Type')[0].lower()\n",
    "            destination_type_str = str(destination_type).split('Type')[0].lower()\n",
    "\n",
    "            if source_type_str in datatype_dict and destination_type_str in datatype_dict:\n",
    "                source_query = datatype_dict[source_type_str].format(system1='source', column_name=col, table1=source_table)\n",
    "                destination_query = datatype_dict[destination_type_str].format(system1='destination', column_name=col, table1=destination_table)\n",
    "                source_query_blank_null = blanks_nulls[source_type_str].format(system='source', column_name=col, table=source_table)\n",
    "                destination_query_blank_null = blanks_nulls[destination_type_str].format(system='destination', column_name=col, table=destination_table)\n",
    "\n",
    "                source_count = spark.sql(source_query).collect()[0][1]\n",
    "                destination_count = spark.sql(destination_query).collect()[0][1]\n",
    "                source_blank_null_count = spark.sql(source_query_blank_null).collect()[0][1]\n",
    "                destination_blank_null_count = spark.sql(destination_query_blank_null).collect()[0][1]\n",
    "\n",
    "\n",
    "                if source_count == destination_count or (destination_count >= (source_count*.90)):\n",
    "                    description = f\"Row count for column {col} matches between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_count, destination_count, True, description))\n",
    "                else:\n",
    "                    description = f\"Row count for column {col} does not match between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_count, destination_count, False, description))\n",
    "\n",
    "                if source_blank_null_count == destination_blank_null_count or (destination_blank_null_count >= (source_blank_null_count*.90)):\n",
    "                    description = f\"Row count for column {col} blank, null, and zero matches between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_blank_null_count, destination_blank_null_count, True, description))\n",
    "                else:\n",
    "                    description = f\"Row count for column {col} blank, null, and zero does not match between {source_table} and {destination_table}\"\n",
    "                    result_df.append((source_table, destination_table, 'Data Validation', col, source_blank_null_count, destination_blank_null_count, False, description))\n",
    "\n",
    "            else:\n",
    "                description = f\"Data type for column {col} is not supported\"\n",
    "                result_df.append((source_table, destination_table, 'Data Validation', col, None, None, False, description))\n",
    "        else:\n",
    "            description = f\"Column {col} not found in {source_table}\"\n",
    "            result_df.append((source_table, destination_table, 'Data Validation', col, None, None, False, description))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab15b5e-0495-4e01-a78c-989c150c0012",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decimal Precision Quality"
    }
   },
   "outputs": [],
   "source": [
    "def decimal_quality(source_type, destination_type):\n",
    "    def is_bad_decimal(data_type):\n",
    "        if data_type is None:\n",
    "            return False\n",
    "        return data_type.typeName() == 'decimal' and data_type.precision < 6\n",
    "\n",
    "    return is_bad_decimal(source_type) or is_bad_decimal(destination_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05faaad8-4a0c-4535-86a3-25477b6618cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Type Check"
    }
   },
   "outputs": [],
   "source": [
    "def DataTypeComparison(source_table, destination_table):\n",
    "    result_df = []\n",
    "\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    destination_schema = spark.table(destination_table).schema\n",
    "\n",
    "    source_columns_types = {field.name: field.dataType for field in source_schema}\n",
    "    destination_columns_types = {field.name: field.dataType for field in destination_schema} #dict to define the column:datatype\n",
    "\n",
    "    for col, dest_type in destination_columns_types.items():\n",
    "        source_type = source_columns_types.get(col)\n",
    "\n",
    "        if source_type is None:\n",
    "            description = f\"{col} is not present in {source_table}\"\n",
    "            result_df.append((source_table, destination_table, 'Data Type Check', col, None, str(dest_type), False, description))\n",
    "        elif source_type != dest_type:\n",
    "            description = f\"Data Type in {source_table}.{col} is not matching with {destination_table}.{col}\"\n",
    "            result_df.append((source_table, destination_table, 'Data Type Check', col, str(source_type), str(dest_type), False, description))\n",
    "        else:\n",
    "            description = f\"Data Type Matched\"\n",
    "            result_df.append((source_table, destination_table, 'Data Type Check', col, str(source_type), str(dest_type), True, description))\n",
    "\n",
    "        if decimal_quality(source_type, dest_type):\n",
    "            description = f\"Source and/or Destination Decimal Type has bad precision\"\n",
    "            result_df.append((source_table, destination_table, 'Data Type Check', col, str(source_type), str(dest_type), False, description))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc6d97b-3cc6-41a5-b6c4-e69716390b59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Function"
    }
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "for index in range(len(Source_list)):\n",
    "    source_table = Source_list[index] #silver.customer_master\n",
    "    destination_table = Destination_list[index] #crm.account\n",
    "\n",
    "    # source table info\n",
    "    source_column_df = spark.sql(\n",
    "        f\"SELECT table_schema, table_name, column_name, full_data_type FROM information_schema.columns WHERE table_schema = '{source_table.split('.')[0].lower()}' AND table_name = '{source_table.split('.')[1].lower()}'\"\n",
    "    )\n",
    "\n",
    "    # destination table info\n",
    "    destination_column_df = spark.sql(\n",
    "        f\"SELECT table_schema, table_name, column_name, full_data_type FROM information_schema.columns WHERE table_schema = '{destination_table.split('.')[0].lower()}' AND table_name = '{destination_table.split('.')[1].lower()}'\"\n",
    "    )\n",
    "\n",
    "    #---------Column existence check---------\n",
    "    column_existence_check = columns_match(source_table, destination_table, source_column_df, destination_column_df)\n",
    "    result_list.extend(column_existence_check)\n",
    "\n",
    "    #--------- Data Type Check ----------\n",
    "    data_type_check = DataTypeComparison(source_table, destination_table) \n",
    "    result_list.extend(data_type_check)\n",
    "\n",
    "    #--------- Data validation  ---------- #need to update mark it as fail it the difference is >10%\n",
    "    data_validation = datavalidation_nulls_blanks(source_table, destination_table, source_column_df, destination_column_df) \n",
    "    result_list.extend(data_validation)\n",
    "\n",
    "# Ensure all elements in result_list are dictionaries with the same keys\n",
    "result_df = spark.createDataFrame(result_list, schema=['Source_Table', 'Destination_Table', 'Check_Type', 'Column_Name', 'Source_DataType', 'Destination_DataType', 'Status', 'Description'])\n",
    "\n",
    "# result_df = result_df.withColumn(\"current_processed_date\", current_timestamp())\n",
    "# result_df.write.mode(\"append\").saveAsTable(\"config.regression_testresults\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc6ad95-8498-438a-96a5-787a47c430d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "result_df = result_df.withColumn(\"current_processed_date\", current_timestamp())\n",
    "result_df.write.mode(\"append\").saveAsTable(\"config.regression_testresults\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7912104309341057,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Regression_Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
