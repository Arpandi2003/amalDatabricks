{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f02b850-a6ce-4376-81ee-49d4760b152a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create a Unique base data set from crm account based on cust_skey for joining to Horizon. This field is only available in Horizon therefore other integration options were not considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a0e5ed-76db-4638-8982-61fc408c8911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using Catalog Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9566250b-1fa1-418a-b120-92d242b72198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../General/NB_Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73252784-254f-4e9a-aa25-8c5cdd848dfa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "catalog initialization"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"use catalog {catalog}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae775d55-139f-4a3f-8faa-89cec3b31f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date of Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15001f27-04b2-40f7-af67-ed89694fc999",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dod"
    }
   },
   "outputs": [],
   "source": [
    "df_death = spark.sql('''\n",
    "                select \n",
    "                Cust_skey, \n",
    "                to_timestamp(cast(RPDOD as string), 'yyyyMMdd') as DateofDeath\n",
    "                from bronze.ods_rmpdem \n",
    "                where currentrecord = 'Yes'\n",
    "                and RPDOD > 0\n",
    "''')\n",
    "df_death.createOrReplaceTempView(\"vw_death\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf53dc6-6dbc-4335-ad13-c1588581cade",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dupes check"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select cust_skey, count(*) from vw_death\n",
    "group by 1\n",
    "having count(*) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd89955b-ae84-4f29-b4b9-64f4fcf9f863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### BusinessEmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524aaf8f-64ee-46c0-b83f-223f480e81bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AOTM Email"
    }
   },
   "outputs": [],
   "source": [
    "df_aotm_email = spark.sql('''\n",
    "                        select \n",
    "                        Company_ID, \n",
    "                        CASE \n",
    "                            WHEN TRIM(Primary_Contact_Email) NOT REGEXP r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$' \n",
    "                            THEN NULL \n",
    "                            ELSE TRIM(Primary_Contact_Email) \n",
    "                        END as BeB_Email  \n",
    "                        from bronze.v_ods_beb_customer\n",
    "                        where CurrentRecord = 'Yes'\n",
    "                          ''')\n",
    "df_aotm_email.createOrReplaceTempView(\"vw_aotm_email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f609c277-be37-4b47-ab66-ea1747b1aeaa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Horizon Business Email"
    }
   },
   "outputs": [],
   "source": [
    "df_horizon_business_email = spark.sql('''\n",
    "select cust_skey, email as Hzn_email\n",
    "from \n",
    "(SELECT \n",
    "      CASE \n",
    "        WHEN TRIM(rminet.riiadr) NOT REGEXP r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$' \n",
    "        THEN NULL \n",
    "        ELSE TRIM(rminet.riiadr) \n",
    "      END AS Email,\n",
    "      mast.cust_skey,\n",
    "      ROW_NUMBER() OVER (PARTITION BY mast.cust_skey ORDER BY rimndt DESC) AS rownum\n",
    "    FROM bronze.ods_rminet rminet\n",
    "    INNER JOIN (\n",
    "      SELECT Cust_Skey \n",
    "      FROM bronze.ods_rmmast \n",
    "      WHERE RMCTYP = 'N' \n",
    "        AND CurrentRecord = 'Yes' \n",
    "    ) mast ON rminet.CUST_KEY = mast.Cust_Skey\n",
    "    where rminet.CurrentRecord = 'Yes'\n",
    "    AND rminet.riityp = 'EML'\n",
    "    )tab \n",
    "\n",
    "    where tab.rownum = 1\n",
    "''')\n",
    "df_horizon_business_email.createOrReplaceTempView(\"vw_horizon_business_email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf1ce72-7131-4c6e-af23-bbe24024555d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "BusinessEmail VIew"
    }
   },
   "outputs": [],
   "source": [
    "df_beb_hzn_business_email =spark.sql('''\n",
    "WITH RankedEmails AS (\n",
    "    SELECT\n",
    "        a.AccountNumber,  \n",
    "        a.Cust_Skey,\n",
    "        a.SF_Cust_Key,\n",
    "        a.Company_ID,\n",
    "        b.BeB_Email,\n",
    "        c.Hzn_email,\n",
    "        ROW_NUMBER() OVER (PARTITION BY a.SF_Cust_Key ORDER BY CASE WHEN b.BeB_Email IS NOT NULL THEN 1 ELSE 2 END) as rn\n",
    "    FROM\n",
    "        (select AccountNumber,  Cust_Skey, SF_Cust_Key , Company_ID\n",
    "                           from silver.customer_master\n",
    "                            where CurrentRecord = 'Yes' )a\n",
    "    LEFT JOIN\n",
    "        vw_aotm_email b ON a.Company_ID = b.Company_ID\n",
    "    LEFT JOIN\n",
    "        vw_horizon_business_email c ON a.Cust_Skey = c.Cust_Skey\n",
    ")\n",
    "SELECT\n",
    "    AccountNumber,\n",
    "    Cust_Skey,\n",
    "    SF_Cust_Key,\n",
    "    Company_ID,\n",
    "    CASE\n",
    "        WHEN BeB_Email IS NOT NULL THEN BeB_Email\n",
    "        ELSE Hzn_email\n",
    "    END AS BusinessEmail\n",
    "FROM\n",
    "    RankedEmails\n",
    "WHERE rn = 1\n",
    "''')\n",
    "df_beb_hzn_business_email.createOrReplaceTempView(\"vw_beb_hzn_business_email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e273c8e0-2ac8-47e5-8534-a3647dbca627",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dupes check"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select sf_cust_key, count(*)  from vw_beb_hzn_business_email\n",
    "group by 1 \n",
    "having count(*) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3fb294d-4533-4d8d-a2e1-94eae86c720f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Employee Flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8309556d-c135-4129-aac9-195b5f70fb04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Employee Flag view"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "\n",
    "df = spark.read.csv(\"/Volumes/ab_uat_catalog/bronze/unstructured/Employee_Flag/employee_flag.csv\", header=True, inferSchema=True).withColumnRenamed('TAX ID', 'TaxId')\n",
    "\n",
    "# Refine TaxId to contain only numbers and valid SSNs\n",
    "df = df.withColumn('TaxId', regexp_extract('TaxId', r'(\\d{3}-\\d{2}-\\d{4}|\\d{9})', 0))\n",
    "\n",
    "# Remove the '-' from the TaxId\n",
    "df = df.withColumn('TaxId', regexp_replace('TaxId', '-', ''))\n",
    "df.createOrReplaceTempView(\"vw_employee_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc55afab-c4eb-4b45-b329-0e316069cd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Create Left Join to CRM.Account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60212dee-42e8-4067-aeba-8c3e871aa70d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join to crm.account"
    }
   },
   "outputs": [],
   "source": [
    "df_final_FA = spark.sql(\n",
    "    \"\"\" select * from (\n",
    "          select a.sf_cust_key,\n",
    "          b.DateofDeath\n",
    "     ,c.BusinessEmail\n",
    "     ,CASE WHEN empl.TaxId IS NOT NULL THEN True ELSE False END AS IsEmployee\n",
    "     ,row_number() over(partition by a.sf_cust_key order by a.ParentID desc) as rownum\n",
    "from \n",
    " silver.customer_master a\n",
    "left join vw_death b on a.cust_skey = b.cust_skey \n",
    "left join vw_beb_hzn_business_email c on a.sf_cust_key = c.SF_Cust_Key\n",
    "left join vw_employee_flag empl on a.DecryptedTaxIDNumber = empl.TaxId\n",
    "\n",
    "Where a.CurrentRecord = 'Yes' and a.Cust_Skey !='' and a.Cust_Skey !=' ' and a.Cust_Skey is not null\n",
    ")\n",
    "where rownum = 1\n",
    "\"\"\"\n",
    ").drop(\"a.DateofDeath\",\"rownum\")\n",
    "\n",
    "df_final_FA.createOrReplaceTempView(\"vw_final_FAvw_final_FA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb92293-285c-43d3-9bcc-525fc35ea0c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from vw_final_FAvw_final_FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bd9b61-957d-4c70-8270-a890e6f57a3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dupes check"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select SF_Cust_key,count(*) from vw_final_FAvw_final_FA\n",
    "group by 1 \n",
    "having count(*) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8cd822-6796-4e63-b7ff-f1bcd7d51d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checks And Write Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c385e82d-459a-44ad-92ea-a8f68bc7d930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Dynamic Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5469bc2-9a31-434e-a244-290cac7d1e8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DestinationSchema = dbutils.widgets.get('DestinationSchema')\n",
    "DestinationTable = dbutils.widgets.get('DestinationTable')\n",
    "AddOnType = dbutils.widgets.get('AddOnType')\n",
    "\n",
    "print(DestinationSchema, DestinationTable, AddOnType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce71d42-5831-4e9e-b891-48eb7d9a8168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_column = spark.read.table(f\"{DestinationSchema}.{DestinationTable}\").columns  # get all the base columns\n",
    "set_addon = df_final_FA.columns  # get only the addon columns\n",
    "get_pk = spark.sql(f\"\"\"select * from config.metadata where lower(DWHTableName)='{DestinationTable}' and lower(DWHSchemaName) ='{DestinationSchema}' \"\"\").collect()[0]['MergeKey']\n",
    "get_pk_temp = get_pk.split(',')  # split the get_pk\n",
    "for pk in get_pk_temp:\n",
    "    set_addon.remove(pk.lower().strip())  # remove pk from the addon\n",
    "excluded_columns = ['Start_Date', 'End_Date', 'DW_Created_By', 'DW_Created_Date', 'DW_Modified_By', 'DW_Modified_Date', 'MergeHashKey', 'CurrentRecord'] + set_addon\n",
    "filtered_basetable_columns = [col for col in base_column if col.lower() not in [ex_col.lower() for ex_col in excluded_columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f7a9d2-3671-4f96-aeb1-c3b9487d8aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get required columns from base table\n",
    "df_base_required = spark.sql(f\"select {','.join(filtered_basetable_columns)} from {DestinationSchema}.{DestinationTable} where currentrecord='Yes' \")\n",
    "df_base_required.createOrReplaceTempView(\"vw_base\")  #use this as a base table\n",
    "\n",
    "if AddOnType == 'AddOn':\n",
    "  if df_base_required.count() > 0:\n",
    "      join_conditions = \" and \".join([f\"vw_base.{col.strip()} = vw_final_FAvw_final_FA.{col.strip()}\" for col in get_pk.split(',')])\n",
    "\n",
    "      df_final_base_with_addon = spark.sql(\n",
    "          f\"\"\"\n",
    "          select\n",
    "              vw_base.*,\n",
    "              {','.join([f'vw_final_FAvw_final_FA.{col} as {col}' for col in set_addon])}\n",
    "          from \n",
    "              vw_base \n",
    "          left join \n",
    "              vw_final_FAvw_final_FA \n",
    "          on \n",
    "              {join_conditions}\n",
    "      \"\"\")\n",
    "      df_final_base_with_addon.createOrReplaceTempView(\"vw_final_base_with_addon\")\n",
    "      print(df_final_base_with_addon.count())\n",
    "  else:\n",
    "    df_final_FA.createOrReplaceTempView(\"vw_final_base_with_addon\")\n",
    "    count = df_final_FA.count()\n",
    "    display(count)\n",
    "else:\n",
    "    df_final_FA.createOrReplaceTempView(\"vw_final_base_with_addon\")\n",
    "    count = df_final_FA.count()\n",
    "    display(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209e562c-312e-4178-a10c-3ac36bfa2f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate the concatenated string\n",
    "base_without_pk = filtered_basetable_columns.copy()\n",
    "for pk in get_pk_temp:\n",
    "    base_without_pk.remove(pk.strip())\n",
    "Mergehashkey_columns = list(set(set_addon + base_without_pk))\n",
    "concatenated_columns = ','.join(Mergehashkey_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a422fd50-4f51-4d49-a2d5-7c75e4236310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the concatenated string in the SQL query\n",
    "query = f\"\"\"\n",
    "select\n",
    " *,\n",
    " MD5(\n",
    "    CONCAT_WS(',', {concatenated_columns})\n",
    "  ) AS MergeHashKey\n",
    "  from\n",
    "  vw_final_base_with_addon\n",
    "\"\"\"\n",
    "df_source = spark.sql(query)\n",
    "set_addon.append('MergeHashKey')\n",
    "set_addon=set(set_addon)\n",
    "df_source.createOrReplaceTempView(\"vw_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32393cc-6c06-4ec8-9d1f-1c131e934612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select \n",
    "  {','.join([f'target.{col}' for col in filtered_basetable_columns]) if spark.sql(f\"SELECT COUNT(*) FROM {DestinationSchema}.{DestinationTable}\").collect()[0][0] > 0 else ','.join([f'source.{col}' for col in get_pk.split(',')])},\n",
    "  {','.join([f'source.{col}' for col in set_addon])},\n",
    "  current_user() as DW_Created_By,\n",
    "  current_timestamp() as DW_Created_Date,\n",
    "  current_user() as DW_Modified_By,\n",
    "  current_timestamp() as DW_Modified_Date,\n",
    "  current_timestamp() as Start_Date,\n",
    "  NULL as End_Date,\n",
    "  'Yes' as CurrentRecord,\n",
    "  CASE \n",
    "    WHEN { ' AND '.join([f'target.{col} IS NULL' for col in get_pk.split(',')]) } THEN 'Insert'\n",
    "    WHEN { ' AND '.join([f'target.{col} = source.{col}' for col in get_pk.split(',')]) } AND source.MergeHashKey != target.MergeHashKey THEN 'Update'\n",
    "    ELSE 'No Changes' \n",
    "  END As Action_Code  \n",
    "from (select {','.join(set_addon)}, {','.join([f'{col}' for col in get_pk.split(',')])} from vw_source group by all) as source\n",
    "left join {DestinationSchema}.{DestinationTable} as target\n",
    "on { ' AND '.join([f'target.{col} = source.{col}' for col in get_pk.split(',')]) }\n",
    "where target.end_date is null\n",
    "\"\"\"\n",
    "\n",
    "df_source = spark.sql(query)\n",
    "df_source = df_source.dropDuplicates()\n",
    "df_source.createOrReplaceTempView(\"vw_silver\")\n",
    "final_col = df_source.columns\n",
    "final_col.remove('Action_Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882e812d-ca23-4941-9b03-328a9564b176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {DestinationSchema}.{DestinationTable}({','.join(final_col)})\n",
    "    SELECT {','.join(final_col)} FROM vw_silver WHERE Action_Code IN ('Insert', 'Update')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {DestinationSchema}.{DestinationTable} AS Target\n",
    "    USING (SELECT {','.join(final_col)} FROM VW_silver WHERE Action_Code='Update') AS Source\n",
    "    ON { ' AND '.join([f'Target.{col} = Source.{col}' for col in get_pk.split(',')])} AND Target.MergeHashKey != Source.MergeHashKey\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "    Target.End_Date = CURRENT_TIMESTAMP(),\n",
    "    Target.DW_Modified_Date = Source.DW_Modified_Date,\n",
    "    Target.DW_Modified_By = Source.DW_Modified_By,\n",
    "    Target.CurrentRecord = 'No'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d4f1ef-c753-4e36-8682-4c29606e0ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * from {DestinationSchema}.{DestinationTable} where End_Date is null\")\n",
    "df.createOrReplaceTempView(\"target_view\")\n",
    "\n",
    "DFSourceNull = spark.sql(f\"\"\"\n",
    "                SELECT t.*,\n",
    "                    CASE WHEN s.{get_pk.split(',')[0]} IS NULL THEN 'No' ELSE 'Yes' END AS CurrentRecordTmp\n",
    "                FROM target_view t\n",
    "                FULL JOIN VW_silver s\n",
    "                ON { ' AND '.join([f's.{col} = t.{col}' for col in get_pk.split(',')]) }\n",
    "            \"\"\")\n",
    "\n",
    "# Filter out the 'DeleteFlag' rows for next steps\n",
    "DFSourceNull.createOrReplaceTempView(\"SourcetoInsertUpdate\")\n",
    "\n",
    "# Merge operation\n",
    "MergeQuery = f\"\"\"\n",
    "        MERGE INTO {DestinationSchema}.{DestinationTable} AS target\n",
    "        USING SourcetoInsertUpdate AS source\n",
    "        ON { ' AND '.join([f'target.{col} = source.{col}' for col in get_pk.split(',')]) }\n",
    "        AND source.CurrentRecordTmp = 'No'\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET target.CurrentRecord = 'Deleted', target.end_date=current_date(), target.DW_modified_Date=current_date(),target.DW_Modified_By='Databricks'\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(MergeQuery)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1665770848526399,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Silver_Account_AddOn",
   "widgets": {
    "AddOnType": {
     "currentValue": "AddOn",
     "nuid": "3802afb7-2f99-497a-b1c5-fbdd792a07fd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "AddOnType",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "AddOnType",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "DestinationSchema": {
     "currentValue": "silver",
     "nuid": "6adf2db9-eb89-4753-8267-894a79746157",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "DestinationSchema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "DestinationSchema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "DestinationTable": {
     "currentValue": "customer_master",
     "nuid": "0a6e0601-5a8f-46e7-aac6-bf7d368b1381",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "DestinationTable",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "DestinationTable",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
